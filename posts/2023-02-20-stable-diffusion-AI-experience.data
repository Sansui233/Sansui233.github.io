[{"_1":2},"routes/posts.$slug",{"_3":4},"data",{"_5":6,"_94":95,"_98":99},"post",{"_7":8,"_9":10,"_11":12,"_13":14,"_15":16,"_17":18,"_23":24,"_25":26,"_27":28,"_29":30,"_81":82,"_84":85,"_86":87,"_92":93},"title","Stable Diffusion AI 绘画的实用经验与总结","slug","2023-02-20-stable-diffusion-AI-experience","date","2023-02-20T16:00:00.000Z","description","AI 画画现状","draft",false,"metadata",{"_19":20,"_21":22},"readingTime",4,"wordCount",944.08,"excerpt","2023-03-03：待大量补充和修订。\n\n进阶经验，非科普。分为五个部分：模型选择，分辨率调整，采样方法，ControlNet，Lora概述。\n此文章涉及多平台发布，由于生成类 AI 的争议，文中无外链推荐。\n一、模型选择\n模型影响构图和笔触和色彩，决定成图的平均水平。社区经常都在换流行的模型，还是根据需求定，huggingface 和 civitai 上很多。\n二次元目前主流三家 Anything Orange Counterfeit 。\n目前有不少混合了 2D 和 3D 的 2.5D 模型，用来出 cos 的","content_html","<p>2023-03-03：待大量补充和修订。</p>\n<hr>\n<p>进阶经验，非科普。分为五个部分：模型选择，分辨率调整，采样方法，ControlNet，Lora概述。</p>\n<p>此文章涉及多平台发布，由于生成类 AI 的争议，文中无外链推荐。</p>\n<h2>一、模型选择</h2>\n<p>模型影响构图和笔触和色彩，决定成图的平均水平。社区经常都在换流行的模型，还是根据需求定，huggingface 和 civitai 上很多。</p>\n<p>二次元目前主流三家 Anything Orange Counterfeit 。</p>\n<p>目前有不少混合了 2D 和 3D 的 2.5D 模型，用来出 cos 的，个人不喜欢。</p>\n<p>下图模型为 pastel-mix，图片画质已严重压缩（看不见则需要开代理，如果你用的 Innoreader 手机版阅读此文，大概率被放头图了）</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/NamiLing/upic/uPic/00153-2596022983-masterpiece,%20best%20quality,%20ultra-detailed,%20illustration,%201boy,%20Link%20in%20Breath%20of%20the%20wild,%20sun%20shine,water,%20on%20back,%20lying%20in%20wa.jpg\" alt=\"pastel-mix\"></p>\n<h3>模型精度选择</h3>\n<p>fp-16(2G)，fp-32(4G)，full(7G) 用于生产没有肉眼可见的区别。full 主要是训练用的。fp16 生产够用了，快，占用小。</p>\n<p>bf-16(2G) 相对 fp-16 会损失一小部分细节，不推荐，会发布 bf-16 版本的模型也较少。</p>\n<h3>VAE权重网络</h3>\n<p>影响色彩，主观效果是增加饱和度（比修图软件饱和度增强好太多）。目前流行的 VAE 就两个，一个 Waifu 的一个 Anything 的，很多带 vae 模型都是用的这两个改了个名字。</p>\n<p>下图为 mix-pro-v3-fp16 模型，叠加了其配套的 WD vae 的 效果。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/NamiLing/upic/uPic/00045-311454338-Original%20Characters,%201boy,%20sea%20shore,%20%20pastel-full.jpg\" alt=\"Mix-pro-v3\"></p>\n<h3>Prompt</h3>\n<p>prompt（提示词）是玄学范围。不同模型的 prompt 词库不一，目前主流的词库有两个</p>\n<ul>\n<li>适用 SD v1.x 及其衍生模型的 lexica</li>\n<li>适用二次元模型的 Danbooru。</li>\n</ul>\n<p>对于二次元模型，Prompt 指定物体，调整动作、指定颜色比较容易。取决于模型的泛化能力。</p>\n<p>Prompt 难以精确控制画风，所以尽管上限非常高，但主流越来越不倾向于用大量 Prompt 叠 buf，转成微调模型以达成精确需求。</p>\n<p>在两个月前关于 Prompt 研究的文档就已经非常多了，这里不做推荐，因为不看文档，直接翻译成英文大概率是有效的。</p>\n<h3>文生图与图生图</h3>\n<p>文生图与图生图本质上没有什么不同，文生图只是将图片输入换成了一张随机的噪声图。</p>\n<h2>二、分辨率</h2>\n<p>不同模型在生成时有最佳的分辨率，大部分都在 512px 左右，不超过 1024px。</p>\n<p>提高分辨率不仅是单纯放大图片，AI 能够用模型细化图片。</p>\n<h3>图生图</h3>\n<p>img2img(图生图) 大部分时候被用来衍生，但是其效果最好的是将 txt2img 的低分辨率图放大（用同一个seed）。</p>\n<p>在 web-ui 中，仅缩放(just-resize)为无 AI 的放大算法处理。 AI 放大使用的 Latent 方法即将被归入 Upscaler，也就是下面说的 Hires fix。</p>\n<h3>Hires fix</h3>\n<p>Hires fix（高清修复）的作用是用 AI 从小图生成高分辨率图，处理用得好可以提高大幅提高图片上限。upscaler（放大器）可选几种放大方法，其中的 Latent 就是 SD 的画图方法，文生图与图生图均是这个方法。</p>\n<p>在 web-ui 中， 默认只有 txt2img 可用，因为 img2img 本身就可以做图片放大。另外，有专门的 upscaler 选项卡。</p>\n<p>使用 Latent upscaler (图生图) 放大到高分辨率图涉及局部重绘，部分模型支持较好，如 Pastel。但在 512 px 完成度就很高的模型上通常没有效果，或者更差（比如炫彩厚涂），这种用 GAN 类 Upscaler 直接放大比较好。</p>\n<p>值得注意的是 web-ui 的 Hires fix 有两版算法，目前 txt2img 是新版， img2img是旧版，效果有一些小差别，很难比较好坏。</p>\n<h2>三、采样方法</h2>\n<p>Sampler（采样方法） 决定笔触和结构的稳定性。常用的有：</p>\n<ul>\n<li>Euler a 是一个规整稳定的采样方法，出的图较平滑</li>\n<li>DDIM 是一个过程不算稳定的采样方法，通常看起来比 Euler a 细节更多</li>\n<li>DPM++ 2M Karas，最近在用的，过程稳定但有细节</li>\n</ul>\n<p>有部分 Sampler 对高分辨率（1024px 以上）支持不好。</p>\n<h2>四、 ControlNet</h2>\n<p>ControlNet 目前是一个需要额外安装的插件，可以用于给线稿上色并细化，或从草图细化。</p>\n<p>你的草图可以决定的是：</p>\n<ul>\n<li>60%-100% 的线条位置（构图）</li>\n<li>主要色块位置。需要搭配图生图，并且色块越碎越好，需要提前做好整体的光影效果。因为 大部分 AI 只会画整，会画碎的比较少（Pastel 会画碎）。</li>\n</ul>\n<p>细化风格（画风）由模型决定，不由你的草稿决定。模型风格对于画风影响非常关键。Prompt 也可以影响画风。</p>\n<p>细化内容（画的具体是什么）由 Prompt 、草稿、模型共同决定，调整 CFG、Denoise 、 ControlNet 的 Weight 可改变三者比例。</p>\n<p>下图为自己的较草的线稿使用 ControlNet 与 Anything V3.0 上色：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/NamiLing/upic/uPic/00275-3225619170-masterpiece,%20best%20quality,%20illustration,face,%20right%20hand,%20atomespheric,%20cold,%20sunshine,%20sky,,%20high%20detail,%20flowers%20and%20leafs.jpg\" alt=\"Control-Net\"></p>\n<p>（这个东西出名是因为某博说 ControlNet 初衷是洗稿……之前 Pixiv 也出过线稿上色的 AI，只是效果好和不好的差别，怎么没人说是搭配 PS 的线稿提取功能洗稿）</p>\n<h2>五、Lora</h2>\n<p>Lora 是一种训练小模型的方法，可以在 15 张图以上生成稳定的单个角色图。</p>\n<p>已有大量角色图积累的流水线漫画苦力会比较省事，搭配 ControlNet 可生成固定姿势，或者使用线稿稳定地上色。但对于连续动作的细微差别不太行，细节演出表现依旧需要依赖线稿，或手动重绘</p>\n<p>训练过程分为裁剪、自动打标、训练，最终生成约 100M 左右的 Lora 网络。Web-ui 已自带。</p>\n<h2>写在后面</h2>\n<p>风格迁移渐渐成熟了，过拟合的问题在逐渐减少。减少过拟合的最终解决思路，还是把端到端的网络，按需求拆成了人为可控的 pipline。个人认为 AI 如果真的要作为工具，模块化的 pipline 是终点。</p>\n<p>直接出成图的是对已有作品的模拟，抽抽卡，融入不到创意的工作流。</p>\n<p>但是在发展好之前，地球可能已经枯竭了。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/NamiLing/upic/uPic/00162-1748765688-masterpiece,%20best%20quality,%20ultra-detailed,%20illustration,%201boy,%20Link%20in%20Breath%20of%20the%20wild,%20stronde%20hair,%20sun%20shine,water,%20emotio.jpg\" alt=\"pastel-mix\"></p>","content_jsx","const{Fragment:n,jsx:i,jsxs:e}=arguments[0];function _createMdxContent(r){const l={h2:\"h2\",h3:\"h3\",hr:\"hr\",img:\"img\",li:\"li\",p:\"p\",ul:\"ul\",...r.components};return e(n,{children:[i(l.p,{children:\"2023-03-03：待大量补充和修订。\"}),\"\\n\",i(l.hr,{}),\"\\n\",i(l.p,{children:\"进阶经验，非科普。分为五个部分：模型选择，分辨率调整，采样方法，ControlNet，Lora概述。\"}),\"\\n\",i(l.p,{children:\"此文章涉及多平台发布，由于生成类 AI 的争议，文中无外链推荐。\"}),\"\\n\",i(l.h2,{id:\"一模型选择\",children:\"一、模型选择\"}),\"\\n\",i(l.p,{children:\"模型影响构图和笔触和色彩，决定成图的平均水平。社区经常都在换流行的模型，还是根据需求定，huggingface 和 civitai 上很多。\"}),\"\\n\",i(l.p,{children:\"二次元目前主流三家 Anything Orange Counterfeit 。\"}),\"\\n\",i(l.p,{children:\"目前有不少混合了 2D 和 3D 的 2.5D 模型，用来出 cos 的，个人不喜欢。\"}),\"\\n\",i(l.p,{children:\"下图模型为 pastel-mix，图片画质已严重压缩（看不见则需要开代理，如果你用的 Innoreader 手机版阅读此文，大概率被放头图了）\"}),\"\\n\",i(l.img,{src:\"https://cdn.jsdelivr.net/gh/NamiLing/upic/uPic/00153-2596022983-masterpiece,%20best%20quality,%20ultra-detailed,%20illustration,%201boy,%20Link%20in%20Breath%20of%20the%20wild,%20sun%20shine,water,%20on%20back,%20lying%20in%20wa.jpg\",alt:\"pastel-mix\"}),\"\\n\",i(l.h3,{id:\"模型精度选择\",children:\"模型精度选择\"}),\"\\n\",i(l.p,{children:\"fp-16(2G)，fp-32(4G)，full(7G) 用于生产没有肉眼可见的区别。full 主要是训练用的。fp16 生产够用了，快，占用小。\"}),\"\\n\",i(l.p,{children:\"bf-16(2G) 相对 fp-16 会损失一小部分细节，不推荐，会发布 bf-16 版本的模型也较少。\"}),\"\\n\",i(l.h3,{id:\"vae权重网络\",children:\"VAE权重网络\"}),\"\\n\",i(l.p,{children:\"影响色彩，主观效果是增加饱和度（比修图软件饱和度增强好太多）。目前流行的 VAE 就两个，一个 Waifu 的一个 Anything 的，很多带 vae 模型都是用的这两个改了个名字。\"}),\"\\n\",i(l.p,{children:\"下图为 mix-pro-v3-fp16 模型，叠加了其配套的 WD vae 的 效果。\"}),\"\\n\",i(l.img,{src:\"https://cdn.jsdelivr.net/gh/NamiLing/upic/uPic/00045-311454338-Original%20Characters,%201boy,%20sea%20shore,%20%20pastel-full.jpg\",alt:\"Mix-pro-v3\"}),\"\\n\",i(l.h3,{id:\"prompt\",children:\"Prompt\"}),\"\\n\",i(l.p,{children:\"prompt（提示词）是玄学范围。不同模型的 prompt 词库不一，目前主流的词库有两个\"}),\"\\n\",e(l.ul,{children:[\"\\n\",i(l.li,{children:\"适用 SD v1.x 及其衍生模型的 lexica\"}),\"\\n\",i(l.li,{children:\"适用二次元模型的 Danbooru。\"}),\"\\n\"]}),\"\\n\",i(l.p,{children:\"对于二次元模型，Prompt 指定物体，调整动作、指定颜色比较容易。取决于模型的泛化能力。\"}),\"\\n\",i(l.p,{children:\"Prompt 难以精确控制画风，所以尽管上限非常高，但主流越来越不倾向于用大量 Prompt 叠 buf，转成微调模型以达成精确需求。\"}),\"\\n\",i(l.p,{children:\"在两个月前关于 Prompt 研究的文档就已经非常多了，这里不做推荐，因为不看文档，直接翻译成英文大概率是有效的。\"}),\"\\n\",i(l.h3,{id:\"文生图与图生图\",children:\"文生图与图生图\"}),\"\\n\",i(l.p,{children:\"文生图与图生图本质上没有什么不同，文生图只是将图片输入换成了一张随机的噪声图。\"}),\"\\n\",i(l.h2,{id:\"二分辨率\",children:\"二、分辨率\"}),\"\\n\",i(l.p,{children:\"不同模型在生成时有最佳的分辨率，大部分都在 512px 左右，不超过 1024px。\"}),\"\\n\",i(l.p,{children:\"提高分辨率不仅是单纯放大图片，AI 能够用模型细化图片。\"}),\"\\n\",i(l.h3,{id:\"图生图\",children:\"图生图\"}),\"\\n\",i(l.p,{children:\"img2img(图生图) 大部分时候被用来衍生，但是其效果最好的是将 txt2img 的低分辨率图放大（用同一个seed）。\"}),\"\\n\",i(l.p,{children:\"在 web-ui 中，仅缩放(just-resize)为无 AI 的放大算法处理。 AI 放大使用的 Latent 方法即将被归入 Upscaler，也就是下面说的 Hires fix。\"}),\"\\n\",i(l.h3,{id:\"hires-fix\",children:\"Hires fix\"}),\"\\n\",i(l.p,{children:\"Hires fix（高清修复）的作用是用 AI 从小图生成高分辨率图，处理用得好可以提高大幅提高图片上限。upscaler（放大器）可选几种放大方法，其中的 Latent 就是 SD 的画图方法，文生图与图生图均是这个方法。\"}),\"\\n\",i(l.p,{children:\"在 web-ui 中， 默认只有 txt2img 可用，因为 img2img 本身就可以做图片放大。另外，有专门的 upscaler 选项卡。\"}),\"\\n\",i(l.p,{children:\"使用 Latent upscaler (图生图) 放大到高分辨率图涉及局部重绘，部分模型支持较好，如 Pastel。但在 512 px 完成度就很高的模型上通常没有效果，或者更差（比如炫彩厚涂），这种用 GAN 类 Upscaler 直接放大比较好。\"}),\"\\n\",i(l.p,{children:\"值得注意的是 web-ui 的 Hires fix 有两版算法，目前 txt2img 是新版， img2img是旧版，效果有一些小差别，很难比较好坏。\"}),\"\\n\",i(l.h2,{id:\"三采样方法\",children:\"三、采样方法\"}),\"\\n\",i(l.p,{children:\"Sampler（采样方法） 决定笔触和结构的稳定性。常用的有：\"}),\"\\n\",e(l.ul,{children:[\"\\n\",i(l.li,{children:\"Euler a 是一个规整稳定的采样方法，出的图较平滑\"}),\"\\n\",i(l.li,{children:\"DDIM 是一个过程不算稳定的采样方法，通常看起来比 Euler a 细节更多\"}),\"\\n\",i(l.li,{children:\"DPM++ 2M Karas，最近在用的，过程稳定但有细节\"}),\"\\n\"]}),\"\\n\",i(l.p,{children:\"有部分 Sampler 对高分辨率（1024px 以上）支持不好。\"}),\"\\n\",i(l.h2,{id:\"四-controlnet\",children:\"四、 ControlNet\"}),\"\\n\",i(l.p,{children:\"ControlNet 目前是一个需要额外安装的插件，可以用于给线稿上色并细化，或从草图细化。\"}),\"\\n\",i(l.p,{children:\"你的草图可以决定的是：\"}),\"\\n\",e(l.ul,{children:[\"\\n\",i(l.li,{children:\"60%-100% 的线条位置（构图）\"}),\"\\n\",i(l.li,{children:\"主要色块位置。需要搭配图生图，并且色块越碎越好，需要提前做好整体的光影效果。因为 大部分 AI 只会画整，会画碎的比较少（Pastel 会画碎）。\"}),\"\\n\"]}),\"\\n\",i(l.p,{children:\"细化风格（画风）由模型决定，不由你的草稿决定。模型风格对于画风影响非常关键。Prompt 也可以影响画风。\"}),\"\\n\",i(l.p,{children:\"细化内容（画的具体是什么）由 Prompt 、草稿、模型共同决定，调整 CFG、Denoise 、 ControlNet 的 Weight 可改变三者比例。\"}),\"\\n\",i(l.p,{children:\"下图为自己的较草的线稿使用 ControlNet 与 Anything V3.0 上色：\"}),\"\\n\",i(l.img,{src:\"https://cdn.jsdelivr.net/gh/NamiLing/upic/uPic/00275-3225619170-masterpiece,%20best%20quality,%20illustration,face,%20right%20hand,%20atomespheric,%20cold,%20sunshine,%20sky,,%20high%20detail,%20flowers%20and%20leafs.jpg\",alt:\"Control-Net\"}),\"\\n\",i(l.p,{children:\"（这个东西出名是因为某博说 ControlNet 初衷是洗稿……之前 Pixiv 也出过线稿上色的 AI，只是效果好和不好的差别，怎么没人说是搭配 PS 的线稿提取功能洗稿）\"}),\"\\n\",i(l.h2,{id:\"五lora\",children:\"五、Lora\"}),\"\\n\",i(l.p,{children:\"Lora 是一种训练小模型的方法，可以在 15 张图以上生成稳定的单个角色图。\"}),\"\\n\",i(l.p,{children:\"已有大量角色图积累的流水线漫画苦力会比较省事，搭配 ControlNet 可生成固定姿势，或者使用线稿稳定地上色。但对于连续动作的细微差别不太行，细节演出表现依旧需要依赖线稿，或手动重绘\"}),\"\\n\",i(l.p,{children:\"训练过程分为裁剪、自动打标、训练，最终生成约 100M 左右的 Lora 网络。Web-ui 已自带。\"}),\"\\n\",i(l.h2,{id:\"写在后面\",children:\"写在后面\"}),\"\\n\",i(l.p,{children:\"风格迁移渐渐成熟了，过拟合的问题在逐渐减少。减少过拟合的最终解决思路，还是把端到端的网络，按需求拆成了人为可控的 pipline。个人认为 AI 如果真的要作为工具，模块化的 pipline 是终点。\"}),\"\\n\",i(l.p,{children:\"直接出成图的是对已有作品的模拟，抽抽卡，融入不到创意的工作流。\"}),\"\\n\",i(l.p,{children:\"但是在发展好之前，地球可能已经枯竭了。\"}),\"\\n\",i(l.img,{src:\"https://cdn.jsdelivr.net/gh/NamiLing/upic/uPic/00162-1748765688-masterpiece,%20best%20quality,%20ultra-detailed,%20illustration,%201boy,%20Link%20in%20Breath%20of%20the%20wild,%20stronde%20hair,%20sun%20shine,water,%20emotio.jpg\",alt:\"pastel-mix\"})]})}return{default:function(n={}){const{wrapper:e}=n.components||{};return e?i(e,{...n,children:i(_createMdxContent,{...n})}):_createMdxContent(n)}};","toc",[31,53,65,69,73,77],{"_7":32,"_33":34,"_35":36},"一、模型选择","url","#一模型选择","items",[37,41,45,49],{"_7":38,"_33":39,"_35":40},"模型精度选择","#模型精度选择",[],{"_7":42,"_33":43,"_35":44},"VAE权重网络","#vae权重网络",[],{"_7":46,"_33":47,"_35":48},"Prompt","#prompt",[],{"_7":50,"_33":51,"_35":52},"文生图与图生图","#文生图与图生图",[],{"_7":54,"_33":55,"_35":56},"二、分辨率","#二分辨率",[57,61],{"_7":58,"_33":59,"_35":60},"图生图","#图生图",[],{"_7":62,"_33":63,"_35":64},"Hires fix","#hires-fix",[],{"_7":66,"_33":67,"_35":68},"三、采样方法","#三采样方法",[],{"_7":70,"_33":71,"_35":72},"四、 ControlNet","#四-controlnet",[],{"_7":74,"_33":75,"_35":76},"五、Lora","#五lora",[],{"_7":78,"_33":79,"_35":80},"写在后面","#写在后面",[],"tags",[83],"AI","categories","乱七八糟的分享","keywords",[88,89,90,91],"AI绘画","stable-diffusion","AI绘图","AI画画","permalink","/posts/2023-02-20-stable-diffusion-AI-experience","prevPost",{"_7":96,"_9":97},"图像超分辨率工具推荐","20230822-Super_resolution_models_and_tools","nextPost",{"_7":100,"_9":101},"部署饥荒联机版Linux服务器","dontstarvetogether-linux-server"]
